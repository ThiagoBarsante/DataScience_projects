{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Machine Learning with Apache Spark\n",
    "\n",
    "##### Build - 3 new ML models using Apache Spark framework\n",
    "- Logistic Regression\n",
    "- Random Forest and\n",
    "- GBM\n",
    "\n",
    "## Recap\n",
    "#### Info about model evaluation - accuracy metric vs recall\n",
    "- The global metric accuracy will be used to evaluate the models between all frameworks (xgb, lgbm, sklearn, h2o.ai and Apache Spark)\n",
    "\n",
    "#### The last notebook build ml models using python will provide some additional techniques, such as:\n",
    "- Unbalanced classification and class weight\n",
    "- Smote technique for oversampling the training dataset\n",
    "- Standard Scale vs. default data and \n",
    "- Finally, exchange the global metric accuracy and use recall metric < recall or Sensitivity or True positive rate (TPR) > \n",
    "<br> Recall metric is a better metric than accuracy to evaluate this type of scenario (customer churn)\n",
    "\n",
    "##### Additional info: https://spark.apache.org/\n",
    "\n",
    "#### Starting process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## Data Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# ML Models\n",
    "from  pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "## Metrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session and load data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Customer_support_ML').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/WA_Fn-UseC_-Telco-Customer-Churn.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/WA_Fn-UseC_-Telco-Customer-Churn.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customerID='7590-VHVEG', gender='Female', SeniorCitizen=0, Partner='Yes', Dependents='No', tenure=1, PhoneService='No', MultipleLines='No phone service', InternetService='DSL', OnlineSecurity='No', OnlineBackup='Yes', DeviceProtection='No', TechSupport='No', StreamingTV='No', StreamingMovies='No', Contract='Month-to-month', PaperlessBilling='Yes', PaymentMethod='Electronic check', MonthlyCharges=29.85, TotalCharges='29.85', Churn='No')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load csv into Spark\n",
    "path = 'data/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "dataset = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "## Show 1 record sample\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep to run the ML models with Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- monthly_charges: double (nullable = true)\n",
      " |-- total_charges: float (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- payment_method: string (nullable = false)\n",
      " |-- contract: string (nullable = false)\n",
      " |-- churn: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filter columns, fill values and convert columns for correct data type\n",
    "dataset.createOrReplaceTempView('v_data')\n",
    "\n",
    "df_spark = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    tenure, \n",
    "    MonthlyCharges as monthly_charges, \n",
    "    CAST ((case when TotalCharges == ' ' then 0 else TotalCharges end) as float) as total_charges,\n",
    "    gender, \n",
    "\tcase when PaymentMethod == 'Electronic check' then 'ElectronicCheck'\n",
    "\t\twhen PaymentMethod == 'Mailed check' then 'MailedCheck'\n",
    "\t\twhen PaymentMethod == 'Bank transfer (automatic)' then 'BankTransferAutomatic'\n",
    "\t\twhen PaymentMethod == 'Credit card (automatic)' then 'CreditCardAutomatic'\n",
    "\t\telse 'Not_mapped'\n",
    "\tend as payment_method,\n",
    "\tcase when Contract == 'Month-to-month' then 'MonthToMonth'\n",
    "\t\t when Contract == 'One year' then 'OneYear'\n",
    "\t\t when Contract == 'Two year' then 'TwoYear'\n",
    "\t\telse 'NotMapped'\n",
    "\tend as contract,\n",
    "    CAST ((case when Churn == 'Yes' then 1 else 0 end) as integer) as churn \n",
    "FROM v_data\n",
    "\"\"\")\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read some data from Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tenure=1, monthly_charges=29.85, total_charges=29.850000381469727, gender='Female', payment_method='ElectronicCheck', contract='MonthToMonth', churn=0),\n",
       " Row(tenure=34, monthly_charges=56.95, total_charges=1889.5, gender='Male', payment_method='MailedCheck', contract='OneYear', churn=0),\n",
       " Row(tenure=2, monthly_charges=53.85, total_charges=108.1500015258789, gender='Male', payment_method='MailedCheck', contract='MonthToMonth', churn=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the distribution and labels applied in the Data Prep above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female| 3488|\n",
      "|  Male| 3555|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|    contract|count|\n",
      "+------------+-----+\n",
      "|     OneYear| 1473|\n",
      "|MonthToMonth| 3875|\n",
      "|     TwoYear| 1695|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby('contract').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|      payment_method|count|\n",
      "+--------------------+-----+\n",
      "|     ElectronicCheck| 2365|\n",
      "|         MailedCheck| 1612|\n",
      "| CreditCardAutomatic| 1522|\n",
      "|BankTransferAutom...| 1544|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby('payment_method').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models build and data pipeline\n",
    "- data pipeline and one hot encode for categorial features \n",
    "- 3 ML models creation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set seed to reproduce similar results between executions\n",
    "SEED = 12345\n",
    "\n",
    "## ML models\n",
    "lm_model = LogisticRegression(featuresCol='features',labelCol='churn')\n",
    "rf_model = RandomForestClassifier(featuresCol='features', labelCol='churn', numTrees=100, seed=SEED)\n",
    "gbm_model = GBTClassifier(featuresCol='features', labelCol='churn', seed=SEED)\n",
    "\n",
    "## split data into train and test for prediction\n",
    "train , test = df_spark.randomSplit([0.75, 0.25], seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT COLUMNs\n",
    "# target = 'Churn'\n",
    "# current_features = ['tenure', 'monthly_charges', 'total_charges', 'gender', 'payment_method' , 'churn', 'contract']\n",
    "\n",
    "gender_idx = StringIndexer(inputCol='gender',outputCol='gender_idx')\n",
    "gender_vec = OneHotEncoder(inputCol='gender_idx', outputCol='gender_vec')\n",
    "\n",
    "contract_idx = StringIndexer(inputCol='contract',outputCol='contract_idx')\n",
    "contract_vec = OneHotEncoder(inputCol='contract_idx', outputCol='contract_vec')\n",
    "\n",
    "payment_method_idx = StringIndexer(inputCol='payment_method', outputCol='payment_method_idx')\n",
    "payment_method_vec = OneHotEncoder(inputCol='payment_method_idx', outputCol='payment_method_vec')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['tenure', 'monthly_charges', 'total_charges', 'gender_vec', \n",
    "                                      'payment_method_vec' , 'contract_vec'\n",
    "                                      ], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML - Logistic Regression\n",
    "- Accuracy: 79,32%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy: 0.7932960893854749\n"
     ]
    }
   ],
   "source": [
    "## data pipeline\n",
    "pipeline_lm = Pipeline(stages=[gender_idx, gender_vec,\n",
    "                               payment_method_idx, payment_method_vec,\n",
    "                               contract_idx, contract_vec,\n",
    "                               assembler, lm_model])\n",
    "\n",
    "## Model Creation and prediction\n",
    "fit_model = pipeline_lm.fit(train)\n",
    "predict_lm = fit_model.transform(test)\n",
    "\n",
    "# print(type(predict_lm))\n",
    "# predict_lm.select('tenure', 'monthly_charges', 'total_charges', 'gender', 'payment_method' , 'churn', 'contract', \n",
    "#                   'probability', 'prediction').show()\n",
    "\n",
    "## Model evaluation\n",
    "eval_acc = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='churn', metricName='accuracy')\n",
    "log_acc = eval_acc.evaluate(predict_lm)\n",
    "\n",
    "# type(log_acc)\n",
    "print('Logistic Regression - accuracy: {}'.format(log_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML - Random Forest\n",
    "- Accuracy: 79,72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy: 0.7972067039106145\n"
     ]
    }
   ],
   "source": [
    "## data pipeline\n",
    "pipeline_rf = Pipeline(stages=[gender_idx, gender_vec,\n",
    "                               payment_method_idx, payment_method_vec,\n",
    "                               contract_idx, contract_vec,\n",
    "                               assembler, rf_model])\n",
    "\n",
    "## Model Creation and prediction\n",
    "fit_model = pipeline_rf.fit(train)\n",
    "predict_rf = fit_model.transform(test)\n",
    "# print(type(predict_rf))\n",
    "# predict_rf.select('tenure', 'monthly_charges', 'total_charges', 'gender', 'payment_method' , 'churn', 'contract', \n",
    "#                   'probability', 'prediction').show()\n",
    "\n",
    "## Metrics - accuracy evaluation\n",
    "eval_acc = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='churn', metricName='accuracy')\n",
    "log_acc = eval_acc.evaluate(predict_rf)\n",
    "\n",
    "# type(log_acc)\n",
    "print('Random Forest - accuracy: {}'.format(log_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML - GBM\n",
    "- Accuracy: 79,83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM - accuracy: 0.7983240223463687\n"
     ]
    }
   ],
   "source": [
    "## data pipeline\n",
    "pipeline_gbm = Pipeline(stages=[gender_idx, gender_vec,\n",
    "                               payment_method_idx, payment_method_vec,\n",
    "                               contract_idx, contract_vec,\n",
    "                               assembler, gbm_model])\n",
    "\n",
    "## Model Creation and prediction\n",
    "fit_model = pipeline_gbm.fit(train)\n",
    "predict_gbm = fit_model.transform(test)\n",
    "\n",
    "# print(type(predict_gbm))\n",
    "# predict_gbm.select('tenure', 'monthly_charges', 'total_charges', 'gender', 'payment_method' , 'churn', 'contract', \n",
    "#                   'probability', 'prediction').show()\n",
    "\n",
    "## Model evaluation\n",
    "eval_acc = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='churn', metricName='accuracy')\n",
    "log_acc = eval_acc.evaluate(predict_gbm)\n",
    "\n",
    "# type(log_acc)\n",
    "print('GBM - accuracy: {}'.format(log_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print confusion matrix for GBM\n",
    "- All 3 models have similar accuracy score, however GBM has the bigger one with small higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GBM results...\n",
      "\n",
      "--   Confusion Matrix\n",
      "[[1184  112]\n",
      " [ 249  245]]\n",
      "\n",
      "--   Accuracy\n",
      "0.7983240223463687\n",
      "\n",
      "--   Metrics report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87      1296\n",
      "           1       0.69      0.50      0.58       494\n",
      "\n",
      "    accuracy                           0.80      1790\n",
      "   macro avg       0.76      0.70      0.72      1790\n",
      "weighted avg       0.79      0.80      0.79      1790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Metrics - Classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "## Function to print Confusion Matrix and metrics\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Print metrics\"\"\"\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    confusion_matrix_rpt = confusion_matrix(y_true, y_pred)\n",
    "    accuracy_score_rpt = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print('--   Confusion Matrix')\n",
    "    print(confusion_matrix_rpt)\n",
    "    print('')\n",
    "    print('--   Accuracy')\n",
    "    print(accuracy_score_rpt)\n",
    "    print('')\n",
    "    print('--   Metrics report')\n",
    "    print(report)\n",
    "\n",
    "## Get values to generate the confusion matrix    \n",
    "y_true = np.array(predict_gbm.select('churn').collect())\n",
    "y_pred = np.array(predict_gbm.select('prediction').collect())\n",
    "\n",
    "print('')\n",
    "print('GBM results...')\n",
    "print('')\n",
    "print_confusion_matrix(y_true, y_pred)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary - Apache Spark\n",
    "- These 3 models above provide almost identical results, and GBM provide the best accuracy with 79,83%\n",
    "\n",
    "#### This notebook along with others using Python show the creation of various Machine Learning models\n",
    "- Frameworks: Sklearn, H2O.ai, Apache Spark, LightGBM and XGBoost\n",
    "- Models: GLM-Generalized Linear Model, Logistic Regression, ..., Random Forest, GBM and Xgboost\n",
    "\n",
    "#### Info\n",
    "- Until now the GBM build with sklearn provide better results - 80% of accuracy\n",
    "\n",
    "Let's move on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to html Phase_2_Build_ML_models_with_Python_4x6_Apache_Spark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
